{
 "cells": [
  {
   "source": [
    "<a href=\"https://www.kaggle.com/code/armanasgharpoor1993/coco-image-segmentation?scriptVersionId=138553552\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ],
   "metadata": {},
   "cell_type": "markdown",
   "id": "40c0a48d5fc08979"
  },
  {
   "cell_type": "markdown",
   "id": "4d25de99",
   "metadata": {
    "papermill": {
     "duration": 0.007724,
     "end_time": "2023-08-01T08:10:16.903022",
     "exception": false,
     "start_time": "2023-08-01T08:10:16.895298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Тutorial по сегментации изображений с использованием глубокого обучения**\n",
    "\n",
    "В этом уроке мы исследуем тему сегментации изображений с использованием глубокого обучения. Сегментация изображений — это важная задача компьютерного зрения, которая заключается в разделении изображения на несколько сегментов или областей для облегчения анализа и понимания. Техники глубокого обучения, особенно свёрточные нейронные сети (CNN), продемонстрировали выдающиеся результаты в задачах сегментации изображений, благодаря своей способности обучаться сложным визуальным представлениям непосредственно из данных.\n",
    "\n",
    "## Введение в сегментацию изображений\n",
    "\n",
    "Сегментация изображений играет ключевую роль в различных приложениях компьютерного зрения, включая детекцию объектов, семантическое понимание, медицинскую визуализацию и автономное вождение. Цель сегментации — идентифицировать и очертить значимые области на изображении, что позволяет проводить детальный анализ и понимать визуальное содержание. Присваивая метки или маски отдельным пикселям или группам пикселей, сегментация изображений предоставляет подробное понимание объектов, границ и контекста на изображении.\n",
    "\n",
    "### Типы сегментации изображений\n",
    "\n",
    "Существует несколько типов техник сегментации изображений, каждая из которых решает разные аспекты задачи сегментации:\n",
    "\n",
    "1. **Семантическая сегментация**: Семантическая сегментация присваивает каждому пикселю на изображении метку класса, эффективно классифицируя его в значимые объектные классы. Например, в сценарии автономного вождения пиксели могут быть классифицированы как «автомобиль», «человек» или «фон». Семантическая сегментация предоставляет общее понимание содержания изображения.\n",
    "\n",
    "2. **Сегментация объектов (instance segmentation)**: Сегментация объектов идет дальше семантической сегментации, не только классифицируя пиксели по классам объектов, но и различая отдельные экземпляры объектов на изображении. Например, в сцене с несколькими автомобилями сегментация объектов может разделить каждый автомобиль и присвоить ему уникальную метку.\n",
    "\n",
    "3. **Обнаружение границ**: Обнаружение границ фокусируется на нахождении и выделении границ или краев между различными объектами или областями на изображении. Эта техника помогает точно очертить контуры объектов, что важно для задач, таких как распознавание объектов и анализ формы.\n",
    "\n",
    "4. **Паноптическая сегментация**: Паноптическая сегментация объединяет как семантическую, так и сегментацию объектов, чтобы предоставить более полное понимание изображения. Она присваивает уникальные метки каждому экземпляру, а также выполняет семантическую сегментацию для фоновых областей. Паноптическая сегментация стремится соединить распознавание объектов и понимание общей сцены.\n",
    "\n",
    "### Глубокое обучение для сегментации изображений\n",
    "\n",
    "Техники глубокого обучения произвели революцию в сегментации изображений, автоматически обучая модели извлекать сложные признаки и паттерны непосредственно из данных. Свёрточные нейронные сети (CNN) зарекомендовали себя как особенно эффективные в захвате пространственных зависимостей и иерархических представлений в изображениях, что делает их идеальными для задач сегментации изображений.\n",
    "\n",
    "В последние годы такие архитектуры, как U-Net, PSPNet и Mask R-CNN, достигли передовых результатов в сегментации изображений. Эти архитектуры используют различные техники для улучшения точности сегментации:\n",
    "\n",
    "- **Пропуски соединений**: Пропуски соединений устанавливают прямые связи между слоями энкодера и декодера, что позволяет интегрировать низкоуровневые и высокоуровневые признаки. Это помогает сохранять детализированные данные в процессе увеличения разрешения.\n",
    "\n",
    "- **Дилатированные свёртки**: Дилатированные свёртки (или атрусные свёртки) увеличивают воспринимаемое поле свёрточных слоёв, не теряя пространственного разрешения. Это позволяет моделям захватывать более широкий контекст, сохраняя при этом детализированную информацию.\n",
    "\n",
    "- **Агрегация контекста на нескольких масштабах**: Техники, такие как пулинг, пирамидальная агрегация пулинга и атрусная пирамидальная агрегация (ASPP), используются для агрегации признаков на разных масштабах. Это позволяет модели захватывать как локальный, так и глобальный контекст, улучшая производительность сегментации.\n",
    "\n",
    "Архитектура U-Net, в частности, приобрела большую популярность благодаря своей эффективности в задачах сегментации медицинских изображений. Она использует структуру энкодер-декодер с пропусками соединений, что позволяет точно локализовать и сегментировать объекты. В этом уроке мы сосредоточимся на создании модели U-Net для сегментации изображений.\n",
    "\n",
    "В следующих разделах мы подробно рассмотрим реализацию модели сегментации изображений с использованием архитектуры U-Net. Мы охватим подготовку датасета, построение модели, стратегии обучения, метрики оценки и передовые техники сегментации изображений. В конце этого урока у вас будет полное понимание сегментации изображений с использованием глубокого обучения, и вы сможете применить эти знания для решения сложных задач сегментации в ваших проектах.\n",
    "\n",
    "---\n",
    "\n",
    "Я использовал ту же модель, что и [здесь](https://github.com/H-arshit/UNET-On-COCO/blob/master/Keras_COCO_UNET.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd27ffe1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-08-01T08:10:16.918855Z",
     "iopub.status.busy": "2023-08-01T08:10:16.917616Z",
     "iopub.status.idle": "2023-08-01T08:11:25.040266Z",
     "shell.execute_reply": "2023-08-01T08:11:25.03909Z"
    },
    "papermill": {
     "duration": 68.1332,
     "end_time": "2023-08-01T08:11:25.043126",
     "exception": false,
     "start_time": "2023-08-01T08:10:16.909926",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-26T08:12:00.844280Z",
     "start_time": "2025-02-26T08:12:00.818081Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install pycocotools\n",
    "#!pip install --upgrade scikit-image scipy\n",
    "#!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5a86c1",
   "metadata": {
    "papermill": {
     "duration": 0.009927,
     "end_time": "2023-08-01T08:11:25.063166",
     "exception": false,
     "start_time": "2023-08-01T08:11:25.053239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Setting up the Environment\n",
    "\n",
    "Before we begin, let's set up the environment by installing the necessary libraries and importing the required modules.\n",
    "\n",
    "\n",
    "In this code snippet, we install the `pycocotools` library, which is a Python API for the Microsoft Common Objects in Context (COCO) dataset. We also import various modules and libraries that we will use throughout the tutorial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98cf3da1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T08:11:25.085098Z",
     "iopub.status.busy": "2023-08-01T08:11:25.084779Z",
     "iopub.status.idle": "2023-08-01T08:11:26.356635Z",
     "shell.execute_reply": "2023-08-01T08:11:26.355653Z"
    },
    "papermill": {
     "duration": 1.285437,
     "end_time": "2023-08-01T08:11:26.359137",
     "exception": false,
     "start_time": "2023-08-01T08:11:25.0737",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-26T08:12:00.855600Z",
     "start_time": "2025-02-26T08:12:00.847712Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "from CustomDataGenerator import CustomDataGenerator\n",
    "from helpy import rename_files_and_update_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75f7e12",
   "metadata": {
    "papermill": {
     "duration": 0.009507,
     "end_time": "2023-08-01T08:11:26.378553",
     "exception": false,
     "start_time": "2023-08-01T08:11:26.369046",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Install proper tensorflow-io version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be90c2c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T08:11:26.400669Z",
     "iopub.status.busy": "2023-08-01T08:11:26.399216Z",
     "iopub.status.idle": "2023-08-01T08:11:41.683603Z",
     "shell.execute_reply": "2023-08-01T08:11:41.682382Z"
    },
    "papermill": {
     "duration": 15.297681,
     "end_time": "2023-08-01T08:11:41.686256",
     "exception": false,
     "start_time": "2023-08-01T08:11:26.388575",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-26T08:12:00.855992Z",
     "start_time": "2025-02-26T08:12:00.851231Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip uninstall -y tensorflow-io\n",
    "#!pip install tensorflow-io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0b4ac8",
   "metadata": {
    "papermill": {
     "duration": 0.011401,
     "end_time": "2023-08-01T08:11:41.709498",
     "exception": false,
     "start_time": "2023-08-01T08:11:41.698097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Загрузка и подготовка датасета\n",
    "\n",
    "Для обучения и оценки модели сегментации изображений нам нужен датасет с аннотированными масками. В этом уроке мы будем использовать датасет COCO, который является крупномасштабным датасетом для детекции объектов, сегментации и генерации подписей.\n",
    "\n",
    "Нам нужно загрузить файлы аннотаций для датасета COCO, которые содержат информацию об экземплярах объектов и их соответствующих масках.\n",
    "\n",
    "В приведённом выше коде мы загружаем файлы аннотаций COCO для обучающего и валидационного наборов данных. Мы указываем интересующие нас классы объектов (в данном случае, только \"человек\"). Мы извлекаем идентификаторы изображений и категорий, соответствующие указанным классам. Также мы загружаем словари изображений для обоих наборов данных — обучающего и валидационного.\n",
    "\n",
    "После загрузки необходимых данных мы выводим количество изображений для обучения и валидации, а также количество категорий. Мы перемешиваем идентификаторы изображений, чтобы в процессе обучения была введена случайность. Кроме того, мы выбираем подмножество идентификаторов изображений для валидации, чтобы ускорить процесс оценки."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "folder_path_train = 'coco-instance-segmentation-tmj-dataset/images/train'\n",
    "folder_path_test = 'coco-instance-segmentation-tmj-dataset/images/test'\n",
    "folder_path_valid = 'coco-instance-segmentation-tmj-dataset/images/valid'\n",
    "\n",
    "rename_files_and_update_annotations(folder_path_train)\n",
    "# rename_files_and_update_annotations(folder_path_test)\n",
    "rename_files_and_update_annotations(folder_path_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T08:12:19.842008Z",
     "start_time": "2025-02-26T08:12:19.811481Z"
    }
   },
   "id": "ad34a23865b983b5",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11175d62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T08:11:41.734086Z",
     "iopub.status.busy": "2023-08-01T08:11:41.733704Z",
     "iopub.status.idle": "2023-08-01T08:15:08.345587Z",
     "shell.execute_reply": "2023-08-01T08:15:08.344136Z"
    },
    "papermill": {
     "duration": 206.628162,
     "end_time": "2023-08-01T08:15:08.348769",
     "exception": false,
     "start_time": "2023-08-01T08:11:41.720607",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-26T08:14:19.801723Z",
     "start_time": "2025-02-26T08:14:19.781768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "113 2\n",
      "50 2\n",
      "113 50\n"
     ]
    },
    {
     "data": {
      "text/plain": "52"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Load paths for the COCO dataset annotation files\n",
    "ANNOTATION_FILE_TRAIN = 'coco-instance-segmentation-tmj-dataset/images/train/_annotations.coco.json'\n",
    "ANNOTATION_FILE_VAL = 'coco-instance-segmentation-tmj-dataset/images/valid/_annotations.coco.json'\n",
    "\n",
    "# Define the object classes of interest\n",
    "classes = ['head', 'pit']\n",
    "\n",
    "# Initialize COCO instances for training set and load relevant data\n",
    "coco_train = COCO(ANNOTATION_FILE_TRAIN)\n",
    "catIds_train = coco_train.getCatIds(catNms=classes)\n",
    "imgIds_train = coco_train.getImgIds(catIds=catIds_train)\n",
    "imgDict_train = coco_train.loadImgs(imgIds_train)\n",
    "\n",
    "# Initialize COCO instances for validation set and load relevant data\n",
    "coco_val = COCO(ANNOTATION_FILE_VAL)\n",
    "catIds_val = coco_val.getCatIds(catNms=classes)\n",
    "imgIds_val = coco_val.getImgIds(catIds=catIds_val)\n",
    "imgDict_val = coco_val.loadImgs(imgIds_val)\n",
    "\n",
    "# Print the number of training and validation images and categories\n",
    "print(len(imgIds_train), len(catIds_train))\n",
    "print(len(imgIds_val), len(catIds_val))\n",
    "\n",
    "# Shuffle the training and validation image IDs\n",
    "shuffle(imgIds_train)\n",
    "shuffle(imgIds_val)\n",
    "\n",
    "# Select a subset of validation image IDs\n",
    "imgIds_val = imgIds_val[0:100]\n",
    "imgIds_train = imgIds_train[0:500]\n",
    "\n",
    "# Generate the list of file names for training and validation person images\n",
    "train_images_person = [\"train_{0:012d}.jpg\".format(ids) for ids in imgIds_train]\n",
    "val_images_person = [\"valid_{0:012d}.jpg\".format(ids) for ids in imgIds_val]\n",
    "\n",
    "# Print the number of training and validation person images\n",
    "print(len(train_images_person), len(val_images_person))\n",
    "\n",
    "# Generate the list of file names for training person images\n",
    "train_images_person = [\"train_{0:012d}.jpg\".format(ids) for ids in imgIds_train]\n",
    "\n",
    "# Generate the list of file names for validation person images\n",
    "val_images_person = [\"valid_{0:012d}.jpg\".format(ids) for ids in imgIds_val]\n",
    "\n",
    "# Check the number of files in the validation images directory\n",
    "len(os.listdir(\"coco-instance-segmentation-tmj-dataset/images/valid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate the masks for training images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcd0891164b9bc08"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 113 masks\n"
     ]
    }
   ],
   "source": [
    "# Initialize a count variable\n",
    "count = 0 \n",
    "os.makedirs(\"coco-instance-segmentation-tmj-dataset/masks/train\", exist_ok=True)\n",
    "\n",
    "for ID in imgIds_train:\n",
    "\n",
    "    # Set the file path for the mask\n",
    "    file_path = \"coco-instance-segmentation-tmj-dataset/masks/train/train_{0:012d}.jpg\".format(ID)\n",
    "\n",
    "    # Retrieve a random image ID from the training set\n",
    "    sampleImgIds = coco_train.getImgIds(imgIds=[ID])\n",
    "    sampleImgDict = coco_train.loadImgs(sampleImgIds[np.random.randint(0, len(sampleImgIds))])[0]\n",
    "\n",
    "    # Retrieve the annotation IDs and annotations for the image\n",
    "    annIds = coco_train.getAnnIds(imgIds=sampleImgDict['id'], catIds=catIds_train, iscrowd=0)\n",
    "    anns = coco_train.loadAnns(annIds)\n",
    "\n",
    "    # Generate the mask by combining the individual instance masks\n",
    "    mask = coco_train.annToMask(anns[0])\n",
    "    for i in range(len(anns)):\n",
    "        mask = mask | coco_train.annToMask(anns[i])\n",
    "\n",
    "    # Convert the mask to an image and save it\n",
    "    mask = Image.fromarray(mask * 255, mode=\"L\")\n",
    "    mask.save(file_path)\n",
    "    count = count + 1\n",
    "\n",
    "print(\"Created\", count, \"masks\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T08:27:58.530006Z",
     "start_time": "2025-02-26T08:27:58.332273Z"
    }
   },
   "id": "c449ff02c34d50d1",
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate the masks for validation images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b6ed4599fcfed0a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 50 masks\n"
     ]
    }
   ],
   "source": [
    "# Reset the count variable\n",
    "count = 0\n",
    "os.makedirs(\"coco-instance-segmentation-tmj-dataset/masks/valid\", exist_ok=True)\n",
    "\n",
    "for ID in imgIds_val:\n",
    "\n",
    "    file_path = \"coco-instance-segmentation-tmj-dataset/masks/valid/valid_{0:012d}.jpg\".format(ID)\n",
    "\n",
    "    # Retrieve a random image ID from the validation set\n",
    "    sampleImgIds = coco_val.getImgIds(imgIds=[ID])\n",
    "    sampleImgDict = coco_val.loadImgs(sampleImgIds[np.random.randint(0, len(sampleImgIds))])[0]\n",
    "\n",
    "    # Retrieve the annotation IDs and annotations for the image\n",
    "    annIds = coco_val.getAnnIds(imgIds=sampleImgDict['id'], catIds=catIds_val, iscrowd=0)\n",
    "    anns = coco_val.loadAnns(annIds)\n",
    "\n",
    "    # Generate the mask by combining the individual instance masks\n",
    "    mask = coco_val.annToMask(anns[0])\n",
    "    for i in range(len(anns)):\n",
    "        mask = mask | coco_val.annToMask(anns[i])\n",
    "\n",
    "    # Convert the mask to an image and save it\n",
    "    mask = Image.fromarray(mask * 255, mode=\"L\")\n",
    "    mask.save(file_path)\n",
    "\n",
    "    count = count + 1\n",
    "\n",
    "print(\"Created\", count, \"masks\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T08:28:05.134159Z",
     "start_time": "2025-02-26T08:28:05.037474Z"
    }
   },
   "id": "fdf8318aba3ad836",
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "id": "e8252e88",
   "metadata": {
    "papermill": {
     "duration": 0.011535,
     "end_time": "2023-08-01T08:15:08.372744",
     "exception": false,
     "start_time": "2023-08-01T08:15:08.361209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Custom Data Generator\n",
    "\n",
    "To efficiently handle the large amount of image and mask data, we create a custom data generator class. This class will generate batches of preprocessed images and masks for training the UNet model.\n",
    "\n",
    "In the code above, we define a `CustomDataGenerator` class that extends the `keras.utils.Sequence` class. This allows us to use the data generator with the Keras model's training functions.\n",
    "\n",
    "The `CustomDataGenerator` class takes the paths to the directories containing the original images and corresponding masks, as well as the batch size as input. It has attributes to store the image and mask filenames and methods to retrieve the matching filenames between images and masks.\n",
    "\n",
    "The `__len__` method returns the number of batches in the generator, and the `__getitem__` method generates a batch of preprocessed images and masks. Inside the `__getitem__` method, we load the images and masks, resize them to a desired size (128x128 in this case), and convert them to numpy arrays. We also perform normalization by dividing the pixel values by 255.0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27712f00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T08:15:08.440107Z",
     "iopub.status.busy": "2023-08-01T08:15:08.439842Z",
     "iopub.status.idle": "2023-08-01T08:15:12.574303Z",
     "shell.execute_reply": "2023-08-01T08:15:12.573208Z"
    },
    "papermill": {
     "duration": 4.149116,
     "end_time": "2023-08-01T08:15:12.576833",
     "exception": false,
     "start_time": "2023-08-01T08:15:08.427717",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-26T08:14:30.098166Z",
     "start_time": "2025-02-26T08:14:30.092589Z"
    }
   },
   "outputs": [],
   "source": [
    "# Usage\n",
    "images_path = 'coco-instance-segmentation-tmj-dataset/images/train'\n",
    "masks_path = 'coco-instance-segmentation-tmj-dataset/masks/train'\n",
    "batch_size = 4\n",
    "\n",
    "# Create an instance of the CustomDataGenerator\n",
    "train_generator = CustomDataGenerator(images_path, masks_path, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08859b0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T08:15:12.602296Z",
     "iopub.status.busy": "2023-08-01T08:15:12.600786Z",
     "iopub.status.idle": "2023-08-01T08:15:12.690542Z",
     "shell.execute_reply": "2023-08-01T08:15:12.689454Z"
    },
    "papermill": {
     "duration": 0.10453,
     "end_time": "2023-08-01T08:15:12.692873",
     "exception": false,
     "start_time": "2023-08-01T08:15:12.588343",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-26T08:14:33.259260Z",
     "start_time": "2025-02-26T08:14:33.249240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n",
      "113\n",
      "52\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# Print the number of files in the train directory containing original images\n",
    "print(len(os.listdir(\"coco-instance-segmentation-tmj-dataset/images/train\")))\n",
    "\n",
    "# Print the number of files in the mask_train directory containing generated masks\n",
    "print(len(os.listdir(\"coco-instance-segmentation-tmj-dataset/masks/train\")))\n",
    "\n",
    "# Print the number of files in the valid directory containing original images\n",
    "print(len(os.listdir(\"coco-instance-segmentation-tmj-dataset/images/valid\")))\n",
    "\n",
    "# Print the number of files in the mask_valid directory containing generated masks\n",
    "print(len(os.listdir(\"coco-instance-segmentation-tmj-dataset/masks/valid\")))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get the list of mask filenames\n",
    "mask_filenames = [filename for filename in os.listdir(masks_path) if filename.endswith('.jpg')]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T08:14:39.054137Z",
     "start_time": "2025-02-26T08:14:39.048789Z"
    }
   },
   "id": "28dc2a68d998a1f5",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['train_000000000046.jpg',\n 'train_000000000052.jpg',\n 'train_000000000085.jpg',\n 'train_000000000091.jpg',\n 'train_000000000090.jpg',\n 'train_000000000084.jpg',\n 'train_000000000053.jpg',\n 'train_000000000047.jpg',\n 'train_000000000051.jpg',\n 'train_000000000045.jpg',\n 'train_000000000079.jpg',\n 'train_000000000092.jpg',\n 'train_000000000086.jpg',\n 'train_000000000087.jpg',\n 'train_000000000093.jpg',\n 'train_000000000078.jpg',\n 'train_000000000044.jpg',\n 'train_000000000050.jpg',\n 'train_000000000068.jpg',\n 'train_000000000054.jpg',\n 'train_000000000040.jpg',\n 'train_000000000097.jpg',\n 'train_000000000083.jpg',\n 'train_000000000108.jpg',\n 'train_000000000109.jpg',\n 'train_000000000082.jpg',\n 'train_000000000096.jpg',\n 'train_000000000041.jpg',\n 'train_000000000055.jpg',\n 'train_000000000069.jpg',\n 'train_000000000043.jpg',\n 'train_000000000057.jpg',\n 'train_000000000080.jpg',\n 'train_000000000094.jpg',\n 'train_000000000095.jpg',\n 'train_000000000081.jpg',\n 'train_000000000056.jpg',\n 'train_000000000042.jpg',\n 'train_000000000025.jpg',\n 'train_000000000031.jpg',\n 'train_000000000019.jpg',\n 'train_000000000018.jpg',\n 'train_000000000030.jpg',\n 'train_000000000024.jpg',\n 'train_000000000032.jpg',\n 'train_000000000026.jpg',\n 'train_000000000027.jpg',\n 'train_000000000033.jpg',\n 'train_000000000037.jpg',\n 'train_000000000023.jpg',\n 'train_000000000022.jpg',\n 'train_000000000036.jpg',\n 'train_000000000008.jpg',\n 'train_000000000020.jpg',\n 'train_000000000034.jpg',\n 'train_000000000035.jpg',\n 'train_000000000021.jpg',\n 'train_000000000009.jpg',\n 'train_000000000004.jpg',\n 'train_000000000010.jpg',\n 'train_000000000038.jpg',\n 'train_000000000039.jpg',\n 'train_000000000011.jpg',\n 'train_000000000005.jpg',\n 'train_000000000013.jpg',\n 'train_000000000007.jpg',\n 'train_000000000006.jpg',\n 'train_000000000012.jpg',\n 'train_000000000016.jpg',\n 'train_000000000002.jpg',\n 'train_000000000003.jpg',\n 'train_000000000017.jpg',\n 'train_000000000029.jpg',\n 'train_000000000001.jpg',\n 'train_000000000015.jpg',\n 'train_000000000014.jpg',\n 'train_000000000000.jpg',\n 'train_000000000028.jpg',\n 'train_000000000067.jpg',\n 'train_000000000073.jpg',\n 'train_000000000098.jpg',\n 'train_000000000107.jpg',\n 'train_000000000106.jpg',\n 'train_000000000112.jpg',\n 'train_000000000099.jpg',\n 'train_000000000072.jpg',\n 'train_000000000066.jpg',\n 'train_000000000070.jpg',\n 'train_000000000064.jpg',\n 'train_000000000058.jpg',\n 'train_000000000104.jpg',\n 'train_000000000110.jpg',\n 'train_000000000111.jpg',\n 'train_000000000105.jpg',\n 'train_000000000059.jpg',\n 'train_000000000065.jpg',\n 'train_000000000071.jpg',\n 'train_000000000049.jpg',\n 'train_000000000075.jpg',\n 'train_000000000061.jpg',\n 'train_000000000101.jpg',\n 'train_000000000100.jpg',\n 'train_000000000060.jpg',\n 'train_000000000074.jpg',\n 'train_000000000048.jpg',\n 'train_000000000062.jpg',\n 'train_000000000076.jpg',\n 'train_000000000089.jpg',\n 'train_000000000102.jpg',\n 'train_000000000103.jpg',\n 'train_000000000088.jpg',\n 'train_000000000077.jpg',\n 'train_000000000063.jpg']"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_filenames"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T08:14:39.787530Z",
     "start_time": "2025-02-26T08:14:39.781637Z"
    }
   },
   "id": "52cdd17712aef478",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5a8dd21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T08:15:12.718569Z",
     "iopub.status.busy": "2023-08-01T08:15:12.716879Z",
     "iopub.status.idle": "2023-08-01T08:15:13.218587Z",
     "shell.execute_reply": "2023-08-01T08:15:13.217727Z"
    },
    "papermill": {
     "duration": 0.519261,
     "end_time": "2023-08-01T08:15:13.223712",
     "exception": false,
     "start_time": "2023-08-01T08:15:12.704451",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-26T08:14:50.472915Z",
     "start_time": "2025-02-26T08:14:50.299614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask Image Shape: (512, 512)\n",
      "Main Image Shape: (512, 512, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indexOfImage = 25\n",
    "# Assuming the first image in the mask folder is the one to plot\n",
    "first_mask_filename = mask_filenames[indexOfImage]\n",
    "image_filename = os.path.splitext(first_mask_filename)[0] + '.jpg'\n",
    "\n",
    "# Load and plot the mask image\n",
    "mask_image = Image.open(os.path.join(masks_path, first_mask_filename))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(mask_image)\n",
    "plt.title('Mask Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Load and plot the corresponding main image\n",
    "main_image = Image.open(os.path.join(images_path, image_filename))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(main_image)\n",
    "plt.title('Main Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Print the shapes of the images\n",
    "print('Mask Image Shape:', np.array(mask_image).shape)\n",
    "print('Main Image Shape:', np.array(main_image).shape)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4c54d0",
   "metadata": {
    "papermill": {
     "duration": 0.015824,
     "end_time": "2023-08-01T08:15:13.255439",
     "exception": false,
     "start_time": "2023-08-01T08:15:13.239615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Построение модели UNet\n",
    "\n",
    "Далее мы определяем архитектуру модели UNet. Архитектура UNet является популярным выбором для задач сегментации изображений благодаря своей способности захватывать как низкоуровневые, так и высокоуровневые признаки.\n",
    "\n",
    "В приведенном коде мы определяем несколько функций для построения модели UNet. Эти функции используются для создания различных блоков модели: блоков сужения (down blocks), блока узкой части (bottleneck block), блоков расширения (up blocks) и блока выхода (output block).\n",
    "\n",
    "Функция `down_block` принимает входной тензор, количество фильтров, размер ядра, шаги, паддинг, инициализатор ядра и параметры для максимального пулинга. Она применяет два свертки с батч-нормализацией и активационными функциями, а затем выполняет максимальный пулинг. Выходом этой функции является обработанный тензор и пулинговый тензор для пропускающих соединений.\n",
    "\n",
    "Функция `bottle_neck` принимает входной тензор, количество фильтров, размер ядра, шаги, паддинг и инициализатор ядра. Она применяет два свертки с батч-нормализацией и активационными функциями. Выходом этой функции является обработанный тензор.\n",
    "\n",
    "Функция `up_block` принимает входной тензор, количество фильтров, тензор пропускающего соединения из соответствующего блока сужения, размер ядра, шаги, коэффициент апсемплинга, окно для максимального пулинга, паддинг и инициализатор ядра. Она выполняет апсемплинг на входном тензоре, конкатенирует его с тензором пропускающего соединения и применяет два свертки с батч-нормализацией и активационными функциями. Выходом этой функции является обработанный тензор.\n",
    "\n",
    "Функция `output_block` принимает входной тензор, паддинг и инициализатор ядра. Она применяет два свертки, первая с 2 фильтрами и вторая с 1 фильтром, обе с активационными функциями. Выходом этой функции является финальный выходной тензор.\n",
    "\n",
    "Наконец, мы определяем функцию `UNet`, которая создает всю модель UNet, используя ранее определенные блоки. Она принимает форму входных данных как аргумент и возвращает модель.\n",
    "\n",
    "Я использовал похожую модель U-Net, как [здесь](https://github.com/H-arshit/UNET-On-COCO/blob/master/Keras_COCO_UNET.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "071daf59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T08:15:13.288188Z",
     "iopub.status.busy": "2023-08-01T08:15:13.287876Z",
     "iopub.status.idle": "2023-08-01T08:15:13.320744Z",
     "shell.execute_reply": "2023-08-01T08:15:13.319848Z"
    },
    "papermill": {
     "duration": 0.051876,
     "end_time": "2023-08-01T08:15:13.322788",
     "exception": false,
     "start_time": "2023-08-01T08:15:13.270912",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-26T08:14:56.732832Z",
     "start_time": "2025-02-26T08:14:56.719323Z"
    }
   },
   "outputs": [],
   "source": [
    "from Unet import UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab966131",
   "metadata": {
    "papermill": {
     "duration": 0.01522,
     "end_time": "2023-08-01T08:15:13.353408",
     "exception": false,
     "start_time": "2023-08-01T08:15:13.338188",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Компиляция и обучение модели\n",
    "\n",
    "В этом разделе мы скомпилируем и обучим модель UNet для сегментации изображений с использованием TensorFlow и Keras. Мы настроим модель с подходящими функциями потерь, метриками оценки и параметрами оптимизации, чтобы добиться точных результатов сегментации.\n",
    "\n",
    "### 6.1 Компиляция модели\n",
    "\n",
    "Сначала мы создаем экземпляр модели UNet с формой входных данных (128, 128, 3) с помощью функции `UNet`, которую мы определили ранее. Модель будет использоваться для сегментации изображений.\n",
    "\n",
    "Затем мы компилируем модель, используя оптимизатор Adam с коэффициентом обучения 1e-4 и функцией потерь бинарной кросс-энтропии. Функция потерь бинарной кросс-энтропии обычно используется для задач бинарной сегментации, где каждый пиксель классифицируется как принадлежащий целевому объекту или нет.\n",
    "\n",
    "Мы также включаем набор метрик для оценки производительности модели во время обучения. Метрики включают Среднее пересечение (MeanIoU), Точность (Precision), Отзывчивость (Recall), Площадь под кривой (AUC) и Точность (Accuracy). Эти метрики предоставляют ценную информацию о качестве предсказаний сегментации модели.\n",
    "\n",
    "### 6.2 Обучение модели\n",
    "\n",
    "Перед обучением модели мы создаем генератор для валидационных данных с использованием класса `CustomDataGenerator`. Этот генератор будет загружать и подготавливать валидационные изображения и маски в пакетах, что делает процесс оценки во время обучения эффективным.\n",
    "\n",
    "Чтобы определить количество шагов на эпоху для обучения, мы вычисляем общее количество файлов масок для обучения и делим его на размер пакета.\n",
    "\n",
    "Во время обучения мы используем несколько функций обратного вызова для улучшения процесса обучения модели. Обратные вызовы включают:\n",
    "\n",
    "- **Early Stopping (Раннее прекращение)**: Этот обратный вызов следит за потерями на валидации и останавливает процесс обучения, если потери не улучшаются в течение определенного числа эпох (параметр patience). Модель восстановит веса из лучшей эпохи с наименьшими потерями на валидации.\n",
    "- **Model Checkpoint (Контрольная точка модели)**: Этот обратный вызов сохраняет веса модели после каждой эпохи, если потери на валидации улучшаются. Это помогает отслеживать наилучшую модель в процессе обучения.\n",
    "- **Reduce Learning Rate on Plateau (Снижение коэффициента обучения на плато)**: Этот обратный вызов снижает коэффициент обучения на 0.7, если потери на валидации не улучшаются в течение определенного числа эпох (параметр patience). Это помогает точнее настроить модель, когда прогресс замедляется.\n",
    "\n",
    "Наконец, мы обучаем модель с помощью функции `fit`, передавая генератор для обучения, генератор для валидации, количество шагов на эпоху и желаемое количество эпох. Модель будет обучаться на предоставленном датасете, и прогресс обучения будет отображаться.\n",
    "\n",
    "После завершения обучения мы сохраняем окончательную обученную модель в файл с именем \"final_model.h5\" для дальнейшего использования.\n",
    "\n",
    "Скомпилировав и обучив модель с подходящими настройками и метриками оценки, мы можем добиться точных результатов сегментации изображений и получить модель, способную идентифицировать границы объектов на изображениях."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from keras.src.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from keras.src.metrics import MeanIoU, Precision, Recall, AUC\n",
    "from keras.src.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import datetime\n",
    "import math\n",
    "#import keras.backend as K\n",
    "\n",
    "from tensorflow.keras.losses import Loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T08:15:03.814083Z",
     "start_time": "2025-02-26T08:15:03.809184Z"
    }
   },
   "id": "7f68b89be9be29b1",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9faf9d13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T08:15:13.387036Z",
     "iopub.status.busy": "2023-08-01T08:15:13.38674Z",
     "iopub.status.idle": "2023-08-01T14:21:15.311198Z",
     "shell.execute_reply": "2023-08-01T14:21:15.308882Z"
    },
    "papermill": {
     "duration": 21972.060539,
     "end_time": "2023-08-01T14:21:25.430747",
     "exception": false,
     "start_time": "2023-08-01T08:15:13.370208",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-26T08:24:25.035053Z",
     "start_time": "2025-02-26T08:15:07.046034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmitry/Developer/PycharmProjects/test2/.venv/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m16/29\u001B[0m \u001B[32m━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━\u001B[0m \u001B[1m6:31\u001B[0m 30s/step - accuracy: 0.9546 - auc: 0.4923 - loss: 0.2189 - mean_io_u: 0.4971 - precision: 0.0049 - recall: 0.0129"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 48\u001B[0m\n\u001B[1;32m     45\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39moptimizer, loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbinary_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m, metrics\u001B[38;5;241m=\u001B[39mmetrics)\n\u001B[1;32m     47\u001B[0m \u001B[38;5;66;03m# Fit the model with the training generator\u001B[39;00m\n\u001B[0;32m---> 48\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\n\u001B[1;32m     56\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;66;03m# Save the final trained model\u001B[39;00m\n\u001B[1;32m     59\u001B[0m model\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcoco-instance-segmentation-tmj-dataset/models/final_model.h5\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/Developer/PycharmProjects/test2/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    115\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/Developer/PycharmProjects/test2/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001B[0m, in \u001B[0;36mTensorFlowTrainer.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001B[0m\n\u001B[1;32m    369\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, iterator \u001B[38;5;129;01min\u001B[39;00m epoch_iterator:\n\u001B[1;32m    370\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m--> 371\u001B[0m     logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    372\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_end(step, logs)\n\u001B[1;32m    373\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop_training:\n",
      "File \u001B[0;32m~/Developer/PycharmProjects/test2/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001B[0m, in \u001B[0;36mTensorFlowTrainer._make_function.<locals>.function\u001B[0;34m(iterator)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mfunction\u001B[39m(iterator):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[1;32m    217\u001B[0m         iterator, (tf\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mIterator, tf\u001B[38;5;241m.\u001B[39mdistribute\u001B[38;5;241m.\u001B[39mDistributedIterator)\n\u001B[1;32m    218\u001B[0m     ):\n\u001B[0;32m--> 219\u001B[0m         opt_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmulti_step_on_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m opt_outputs\u001B[38;5;241m.\u001B[39mhas_value():\n\u001B[1;32m    221\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m~/Developer/PycharmProjects/test2/.venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/Developer/PycharmProjects/test2/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    830\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 833\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    835\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    836\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/Developer/PycharmProjects/test2/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    875\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    876\u001B[0m \u001B[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001B[39;00m\n\u001B[1;32m    877\u001B[0m \u001B[38;5;66;03m# run the first trace but we should fail if variables are created.\u001B[39;00m\n\u001B[0;32m--> 878\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mtracing_compilation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    879\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_variable_creation_config\u001B[49m\n\u001B[1;32m    880\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    881\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_created_variables:\n\u001B[1;32m    882\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreating variables on a non-first call to a function\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    883\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m decorated with tf.function.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Developer/PycharmProjects/test2/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001B[0m, in \u001B[0;36mcall_function\u001B[0;34m(args, kwargs, tracing_options)\u001B[0m\n\u001B[1;32m    137\u001B[0m bound_args \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mbind(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    138\u001B[0m flat_inputs \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39munpack_inputs(bound_args)\n\u001B[0;32m--> 139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=protected-access\u001B[39;49;00m\n\u001B[1;32m    140\u001B[0m \u001B[43m    \u001B[49m\u001B[43mflat_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Developer/PycharmProjects/test2/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, tensor_inputs, captured_inputs)\u001B[0m\n\u001B[1;32m   1318\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1319\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1320\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1321\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1322\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_preflattened\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1323\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1324\u001B[0m     args,\n\u001B[1;32m   1325\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1326\u001B[0m     executing_eagerly)\n\u001B[1;32m   1327\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m~/Developer/PycharmProjects/test2/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001B[0m, in \u001B[0;36mAtomicFunction.call_preflattened\u001B[0;34m(self, args)\u001B[0m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcall_preflattened\u001B[39m(\u001B[38;5;28mself\u001B[39m, args: Sequence[core\u001B[38;5;241m.\u001B[39mTensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    215\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 216\u001B[0m   flat_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    217\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mpack_output(flat_outputs)\n",
      "File \u001B[0;32m~/Developer/PycharmProjects/test2/.venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001B[0m, in \u001B[0;36mAtomicFunction.call_flat\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m record\u001B[38;5;241m.\u001B[39mstop_recording():\n\u001B[1;32m    250\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n\u001B[0;32m--> 251\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_bound_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    252\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction_type\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflat_outputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    256\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    257\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m make_call_op_in_graph(\n\u001B[1;32m    258\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    259\u001B[0m         \u001B[38;5;28mlist\u001B[39m(args),\n\u001B[1;32m    260\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mfunction_call_options\u001B[38;5;241m.\u001B[39mas_attrs(),\n\u001B[1;32m    261\u001B[0m     )\n",
      "File \u001B[0;32m~/Developer/PycharmProjects/test2/.venv/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1683\u001B[0m, in \u001B[0;36mContext.call_function\u001B[0;34m(self, name, tensor_inputs, num_outputs)\u001B[0m\n\u001B[1;32m   1681\u001B[0m cancellation_context \u001B[38;5;241m=\u001B[39m cancellation\u001B[38;5;241m.\u001B[39mcontext()\n\u001B[1;32m   1682\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cancellation_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1683\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1684\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1685\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1686\u001B[0m \u001B[43m      \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1687\u001B[0m \u001B[43m      \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1688\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1689\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1690\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1691\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m   1692\u001B[0m       name\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   1693\u001B[0m       num_outputs\u001B[38;5;241m=\u001B[39mnum_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1697\u001B[0m       cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_context,\n\u001B[1;32m   1698\u001B[0m   )\n",
      "File \u001B[0;32m~/Developer/PycharmProjects/test2/.venv/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     52\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 53\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     56\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "images_path = 'coco-instance-segmentation-tmj-dataset/images/valid'\n",
    "masks_path = 'coco-instance-segmentation-tmj-dataset/masks/valid'\n",
    "\n",
    "val_generator = CustomDataGenerator(images_path, masks_path, batch_size)\n",
    "\n",
    "# Fit the model with the training generator\n",
    "\n",
    "# Correcting the calculation of train_steps\n",
    "train_steps = math.ceil(len(os.listdir('coco-instance-segmentation-tmj-dataset/masks/train')) / batch_size)\n",
    "\n",
    "\n",
    "\n",
    "# Define the paths for saving model checkpoints and logs\n",
    "checkpoint_path = 'coco-instance-segmentation-tmj-dataset/checkpoints/model_checkpoint.h5'\n",
    "log_dir = 'coco-instance-segmentation-tmj-dataset/logs'\n",
    "\n",
    "# Define the number of epochs and batch size\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=10, restore_best_weights=True),\n",
    "    ModelCheckpoint(checkpoint_path, save_best_only=True),\n",
    "    ReduceLROnPlateau(factor=0.7, patience=5)\n",
    "]\n",
    "\n",
    "# Create a timestamp for unique log directory\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = log_dir + current_time\n",
    "\n",
    "# Define loss function, metrics, and optimizer\n",
    "metrics = [\n",
    "    MeanIoU(num_classes=3),\n",
    "    Precision(),\n",
    "    Recall(),\n",
    "    AUC(),\n",
    "    'accuracy'\n",
    "]\n",
    "optimizer = Adam(learning_rate=1e-3)\n",
    "\n",
    "# Compile the model\n",
    "model = UNet(input_shape = (512,512,3))\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "\n",
    "# Fit the model with the training generator\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the final trained model\n",
    "model.save('coco-instance-segmentation-tmj-dataset/models/final_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1c733b",
   "metadata": {
    "papermill": {
     "duration": 10.738753,
     "end_time": "2023-08-01T14:21:46.962332",
     "exception": false,
     "start_time": "2023-08-01T14:21:36.223579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Making Predictions and Visualizing Results\n",
    "\n",
    "After training the UNet model, we can now use it to make predictions on new images and visualize the segmentation results. In this section, we will load a sample batch of images from the validation data generator and generate predictions using the trained model.\n",
    "\n",
    "### 7.1 Making Predictions\n",
    "\n",
    "To start, we obtain a sample batch of images and their corresponding masks from the validation data generator. The `val_generator[0]` function call fetches a batch of images and masks.\n",
    "\n",
    "We then use the trained model to generate predictions on this sample batch of images. The `model.predict` function applies the model to the images and returns the predicted masks.\n",
    "\n",
    "### 7.2 Thresholding Predictions\n",
    "\n",
    "The model's predictions are usually represented as continuous probability maps. To convert these probabilities into binary masks, we apply a thresholding operation. In this case, we use a threshold of 0.7 (adjustable based on requirements). Pixels with probabilities greater than 0.7 will be considered as foreground (belonging to the target object), and pixels with probabilities less than or equal to 0.7 will be considered as background.\n",
    "\n",
    "### 7.3 Visualizing Results\n",
    "\n",
    "We randomly select an index from the batch to visualize the results for a single sample image. The selected sample image, ground truth mask, and the predicted mask (after thresholding) are plotted side by side for comparison.\n",
    "\n",
    "To achieve this, we create a figure with three subplots, displaying the sample image, ground truth mask, and predicted mask. Each subplot is annotated with its corresponding title for clarity.\n",
    "\n",
    "We adjust the spacing between the subplots to ensure they are well-organized in the figure. The final figure is displayed using `plt.show()`.\n",
    "\n",
    "By following these steps, we can easily visualize the model's segmentation performance on sample images and gain insights into how well it identifies the target objects within the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62b68e0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T14:22:08.048175Z",
     "iopub.status.busy": "2023-08-01T14:22:08.047105Z",
     "iopub.status.idle": "2023-08-01T14:22:09.753781Z",
     "shell.execute_reply": "2023-08-01T14:22:09.75284Z"
    },
    "papermill": {
     "duration": 12.28274,
     "end_time": "2023-08-01T14:22:09.757545",
     "exception": false,
     "start_time": "2023-08-01T14:21:57.474805",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-26T08:12:00.971906Z",
     "start_time": "2025-02-26T08:12:00.942392Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Get a sample batch from the validation data generator\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m sample_images, sample_masks \u001B[38;5;241m=\u001B[39m \u001B[43mval_generator\u001B[49m[\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Generate predictions on the sample batch\u001B[39;00m\n\u001B[1;32m      5\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(sample_images)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'val_generator' is not defined"
     ]
    }
   ],
   "source": [
    "# Get a sample batch from the validation data generator\n",
    "sample_images, sample_masks = val_generator[2]\n",
    "\n",
    "# Generate predictions on the sample batch\n",
    "predictions = model.predict(sample_images)\n",
    "\n",
    "# Threshold the predictions (if needed)\n",
    "threshold = 0.5  # Adjust the threshold as per your requirement\n",
    "thresholded_predictions = (predictions > threshold).astype(np.uint8)\n",
    "\n",
    "# Select a random index from the batch\n",
    "idx = np.random.randint(0, sample_images.shape[0])\n",
    "\n",
    "# Plot the sample image, ground truth mask, and predicted mask\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# Plot sample image\n",
    "axes[0].imshow(sample_images[idx])\n",
    "axes[0].set_title('Sample Image')\n",
    "\n",
    "# Plot ground truth mask\n",
    "axes[1].imshow(sample_masks[idx])\n",
    "axes[1].set_title('Ground Truth Mask')\n",
    "\n",
    "# Plot predicted mask\n",
    "axes[2].imshow(thresholded_predictions[idx])\n",
    "axes[2].set_title('Predicted Mask')\n",
    "\n",
    "# Set common title for the figure\n",
    "fig.suptitle('Sample Image, Ground Truth Mask, and Predicted Mask')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69971d5",
   "metadata": {
    "papermill": {
     "duration": 10.586042,
     "end_time": "2023-08-01T14:22:31.294058",
     "exception": false,
     "start_time": "2023-08-01T14:22:20.708016",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Visualizing Segmentation Results with Overlaid Masks\n",
    "\n",
    "After obtaining predictions from the trained UNet model, we can visually compare the segmentation performance by overlaying the predicted masks on the sample images. This will allow us to see how well the model identifies the target objects and how closely the predicted masks align with the ground truth masks.\n",
    "\n",
    "### 8.1 Generating Predictions\n",
    "\n",
    "First, we fetch a sample batch of images and their corresponding masks from the validation data generator using `val_generator[0]`.\n",
    "\n",
    "Next, we use the trained model to generate predictions on this sample batch of images. The `model.predict` function applies the model to the images and returns the predicted masks.\n",
    "\n",
    "### 8.2 Thresholding Predictions\n",
    "\n",
    "Similar to the previous section, we apply a thresholding operation to convert the continuous probability maps into binary masks. This time, we use a lower threshold of 0.3 (adjustable based on requirements). Pixels with probabilities greater than 0.3 will be considered as foreground, and pixels with probabilities less than or equal to 0.3 will be considered as background.\n",
    "\n",
    "### 8.3 Visualizing Results with Overlaid Masks\n",
    "\n",
    "To visualize the results, we select a random index from the batch to focus on a single sample image. We then create a figure with three subplots: the sample image, the ground truth mask, and the predicted mask.\n",
    "\n",
    "For the ground truth mask subplot, we use `imshow` with the 'jet' colormap and set the alpha value to 0.7. This allows us to overlay the ground truth mask on the sample image with transparency, making it easy to compare the model's segmentation with the actual masks.\n",
    "\n",
    "Similarly, for the predicted mask subplot, we again use `imshow` with the 'jet' colormap and set the alpha value to 0.7 to overlay the predicted mask on the sample image.\n",
    "\n",
    "The figure is annotated with titles for each subplot, and a common title is set to summarize the purpose of the figure.\n",
    "\n",
    "We adjust the spacing between subplots to ensure a neat layout, and then display the figure using `plt.show()`.\n",
    "\n",
    "By following these steps, we can visually assess the quality of the model's segmentation by comparing the overlaid predicted masks with the ground truth masks and sample images. This visualization provides valuable insights into the model's performance and can help in fine-tuning the segmentation results if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40c5e4bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T14:22:52.576648Z",
     "iopub.status.busy": "2023-08-01T14:22:52.576228Z",
     "iopub.status.idle": "2023-08-01T14:23:03.597385Z",
     "shell.execute_reply": "2023-08-01T14:23:03.596497Z"
    },
    "papermill": {
     "duration": 21.788801,
     "end_time": "2023-08-01T14:23:03.602528",
     "exception": false,
     "start_time": "2023-08-01T14:22:41.813727",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-26T08:12:01.065498Z",
     "start_time": "2025-02-26T08:12:01.064443Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Get a sample batch from the validation data generator\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m10\u001B[39m):\n\u001B[0;32m----> 3\u001B[0m     sample_images, sample_masks \u001B[38;5;241m=\u001B[39m \u001B[43mval_generator\u001B[49m[i]\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;66;03m# Generate predictions on the sample batch\u001B[39;00m\n\u001B[1;32m      6\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(sample_images)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'val_generator' is not defined"
     ]
    }
   ],
   "source": [
    "# Get a sample batch from the validation data generator\n",
    "for i in range(10):\n",
    "    sample_images, sample_masks = val_generator[i]\n",
    "\n",
    "    # Generate predictions on the sample batch\n",
    "    predictions = model.predict(sample_images)\n",
    "\n",
    "    # Threshold the predictions (if needed)\n",
    "    threshold = 0.5  # Adjust the threshold as per your requirement\n",
    "    thresholded_predictions = (predictions > threshold).astype(np.uint8)\n",
    "\n",
    "    # Select a random index from the batch\n",
    "    idx = np.random.randint(0, sample_images.shape[0])\n",
    "\n",
    "    # Plot the sample image, ground truth mask, and predicted mask\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "    # Plot sample image\n",
    "    axes[0].imshow(sample_images[idx])\n",
    "    axes[0].set_title('Sample Image')\n",
    "\n",
    "    # Plot ground truth mask\n",
    "    axes[1].imshow(sample_images[idx])\n",
    "    axes[1].imshow(sample_masks[idx], cmap='jet', alpha=0.7)\n",
    "    axes[1].set_title('Ground Truth Mask')\n",
    "\n",
    "    # Plot predicted mask\n",
    "    axes[2].imshow(sample_images[idx])\n",
    "    axes[2].imshow(thresholded_predictions[idx], cmap='jet', alpha=0.7)\n",
    "    axes[2].set_title('Predicted Mask')\n",
    "\n",
    "    # Set common title for the figure\n",
    "    fig.suptitle('Sample Image, Ground Truth Mask, and Predicted Mask')\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the figure\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22390.706743,
   "end_time": "2023-08-01T14:23:17.237027",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-01T08:10:06.530284",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
